#!/usr/bin/env python3
"""
ê¸°ì¡´ CSV íŒŒì¼ë“¤ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í˜„ì¬ ì¡´ì¬í•˜ëŠ” ëª¨ë“  CSV íŒŒì¼ì— ëŒ€í•´ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
Phase 1 êµ¬í˜„ì˜ ì¼ë¶€ë¡œ, ì¦ë¶„ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ê¸°ë°˜ì„ ë§ˆë ¨í•©ë‹ˆë‹¤.
"""

import sys
import os
from pathlib import Path
import logging
from typing import List, Tuple

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.metadata_manager import MetadataManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def find_csv_files(data_dir: Path) -> List[Tuple[str, str, Path]]:
    """
    ë°ì´í„° ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  CSV íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.

    Args:
        data_dir (Path): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ

    Returns:
        List[Tuple[str, str, Path]]: (feature_path, code, csv_path) ë¦¬ìŠ¤íŠ¸
    """
    csv_files = []

    for csv_path in data_dir.rglob("*.csv"):
        # ë©”íƒ€ë°ì´í„° ë””ë ‰í† ë¦¬ëŠ” ì œì™¸
        if ".metadata" in str(csv_path):
            continue

        # ìƒëŒ€ ê²½ë¡œ ê³„ì‚°
        try:
            relative_to_data = csv_path.relative_to(data_dir)
            feature_path = str(relative_to_data.parent)
            code = csv_path.stem

            csv_files.append((feature_path, code, csv_path))

        except ValueError:
            logger.warning(f"CSV íŒŒì¼ ê²½ë¡œ ê³„ì‚° ì‹¤íŒ¨: {csv_path}")
            continue

    return csv_files


def detect_date_column(csv_path: Path) -> str:
    """
    CSV íŒŒì¼ì—ì„œ ë‚ ì§œ ì»¬ëŸ¼ì„ ìë™ ê°ì§€í•©ë‹ˆë‹¤.

    Args:
        csv_path (Path): CSV íŒŒì¼ ê²½ë¡œ

    Returns:
        str: ê°ì§€ëœ ë‚ ì§œ ì»¬ëŸ¼ëª… ë˜ëŠ” ê¸°ë³¸ê°’
    """
    try:
        import pandas as pd

        df = pd.read_csv(csv_path, nrows=5)  # ì²˜ìŒ 5í–‰ë§Œ ì½ê¸°

        # ê°€ëŠ¥í•œ ë‚ ì§œ ì»¬ëŸ¼ëª…ë“¤
        date_columns = [
            "trade_date",
            "stnd_dt",
            "date",
            "trading_date",
            "stck_bsop_date",
            "bsns_date",
        ]

        for col in date_columns:
            if col in df.columns:
                return col

        # ë‚ ì§œ íŒ¨í„´ì„ ê°€ì§„ ì»¬ëŸ¼ ì°¾ê¸°
        for col in df.columns:
            if "date" in col.lower() or "dt" in col.lower():
                return col

        # ê¸°ë³¸ê°’
        return "trade_date"

    except Exception as e:
        logger.warning(f"ë‚ ì§œ ì»¬ëŸ¼ ê°ì§€ ì‹¤íŒ¨: {e}")
        return "trade_date"


def determine_feature_name(feature_path: str) -> str:
    """
    ê²½ë¡œì—ì„œ í”¼ì²˜ëª…ì„ ê²°ì •í•©ë‹ˆë‹¤.

    Args:
        feature_path (str): í”¼ì²˜ ê²½ë¡œ

    Returns:
        str: í”¼ì²˜ëª…
    """
    # ê²½ë¡œì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ í”¼ì²˜ëª…ìœ¼ë¡œ ì‚¬ìš©
    return Path(feature_path).name


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™” ì‹œì‘")

    # ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •
    data_dir = project_root / "data"
    if not data_dir.exists():
        logger.error(f"ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_dir}")
        return

    # MetadataManager ì´ˆê¸°í™”
    metadata_manager = MetadataManager(str(data_dir))

    # CSV íŒŒì¼ ì°¾ê¸°
    logger.info("ğŸ“‚ CSV íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
    csv_files = find_csv_files(data_dir)

    if not csv_files:
        logger.warning("CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    logger.info(f"ğŸ“Š {len(csv_files)}ê°œì˜ CSV íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

    # ê° CSV íŒŒì¼ì— ëŒ€í•´ ë©”íƒ€ë°ì´í„° ìƒì„±
    success_count = 0
    total_count = len(csv_files)

    for feature_path, code, csv_path in csv_files:
        try:
            logger.info(f"ğŸ” ì²˜ë¦¬ ì¤‘: {feature_path}/{code}.csv")

            # í”¼ì²˜ëª… ê²°ì •
            feature_name = determine_feature_name(feature_path)

            # ë‚ ì§œ ì»¬ëŸ¼ ê°ì§€
            date_column = detect_date_column(csv_path)

            # ë©”íƒ€ë°ì´í„° ìƒì„±
            update_info = metadata_manager.create_update_info(
                feature_name=feature_name,
                code=code,
                csv_path=csv_path,
                date_column=date_column,
            )

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            if metadata_manager.save_last_update_info(feature_path, code, update_info):
                # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                metadata_manager.add_to_history(feature_path, code, update_info)

                success_count += 1
                logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ: {feature_path}/{code}")

                # ìƒì„±ëœ ì •ë³´ ìš”ì•½ ì¶œë ¥
                date_range = update_info.get("date_range", {})
                logger.info(
                    f"   ğŸ“… ë°ì´í„° ë²”ìœ„: {date_range.get('start')} ~ {date_range.get('end')}"
                )
                logger.info(f"   ğŸ“ˆ ì´ ë ˆì½”ë“œ: {update_info.get('total_records')}ê°œ")

            else:
                logger.error(f"âŒ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {feature_path}/{code}")

        except Exception as e:
            logger.error(f"âŒ ë©”íƒ€ë°ì´í„° ìƒì„± ì˜¤ë¥˜ ({feature_path}/{code}): {e}")

    # ê²°ê³¼ ìš”ì•½
    logger.info("=" * 50)
    logger.info(f"ğŸ ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ")
    logger.info(f"âœ… ì„±ê³µ: {success_count}/{total_count}ê°œ")

    if success_count < total_count:
        logger.warning(f"âš ï¸  ì‹¤íŒ¨: {total_count - success_count}ê°œ")

    # ìƒì„±ëœ ë©”íƒ€ë°ì´í„° ë””ë ‰í† ë¦¬ ì •ë³´
    metadata_dirs = []
    for root, dirs, files in os.walk(data_dir):
        if ".metadata" in dirs:
            metadata_dirs.append(Path(root) / ".metadata")

    logger.info(f"ğŸ“ ë©”íƒ€ë°ì´í„° ë””ë ‰í† ë¦¬: {len(metadata_dirs)}ê°œ ìƒì„±")
    for md_dir in metadata_dirs:
        file_count = len(list(md_dir.glob("*.json")))
        logger.info(f"   {md_dir.relative_to(data_dir)}: {file_count}ê°œ íŒŒì¼")


if __name__ == "__main__":
    main()
