"""
Phase 3: API ìµœì í™” ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ë‹¤ìŒ ê¸°ëŠ¥ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤:
1. ì§€ëŠ¥í˜• ìš”ì²­ ë°°ì¹˜ ì²˜ë¦¬
2. ì ì‘í˜• ì†ë„ ì œí•œ ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
3. API ì‘ë‹µ ìºì‹± ì‹œìŠ¤í…œ
4. ë™ì  ë‚ ì§œ ë²”ìœ„ ë¶„í• 
5. ìš”ì²­ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
"""

import asyncio
import time
import logging
import hashlib
import json
from typing import Dict, List, Any, Optional, Tuple, Union, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from collections import defaultdict, deque
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import pickle
import os

logger = logging.getLogger(__name__)


@dataclass
class APIRequest:
    """API ìš”ì²­ ì •ë³´ë¥¼ ë‹´ëŠ” í´ë˜ìŠ¤"""

    api_name: str
    method: str = "GET"
    tr_id: Optional[str] = None
    params: Optional[Dict[str, Any]] = None
    body: Optional[Dict[str, Any]] = None
    headers: Optional[Dict[str, Any]] = None
    priority: int = 5  # 1(ìµœê³ ) ~ 10(ìµœì €)
    timeout: int = 30
    retry_count: int = 3
    cache_key: Optional[str] = None
    created_at: datetime = field(default_factory=datetime.now)


@dataclass
class APIResponse:
    """API ì‘ë‹µ ì •ë³´ë¥¼ ë‹´ëŠ” í´ë˜ìŠ¤"""

    request: APIRequest
    data: Dict[str, Any]
    status_code: int
    response_time: float
    cached: bool = False
    error: Optional[str] = None
    timestamp: datetime = field(default_factory=datetime.now)


class RateLimiter:
    """ì ì‘í˜• ì†ë„ ì œí•œê¸°"""

    def __init__(self, max_requests: int = 100, per_seconds: int = 60):
        self.max_requests = max_requests
        self.per_seconds = per_seconds
        self.requests = deque()
        self.lock = threading.Lock()
        self._adaptive_delay = 0.0
        self._last_error_time = None
        self._consecutive_errors = 0

    def acquire(self) -> float:
        """ìš”ì²­ í—ˆê°€ë¥¼ ì–»ê³  ëŒ€ê¸° ì‹œê°„ì„ ë°˜í™˜"""
        with self.lock:
            now = time.time()

            # ì˜¤ë˜ëœ ìš”ì²­ ê¸°ë¡ ì œê±°
            while self.requests and self.requests[0] <= now - self.per_seconds:
                self.requests.popleft()

            # ìš”ì²­ í•œë„ í™•ì¸
            if len(self.requests) >= self.max_requests:
                wait_time = (
                    self.per_seconds - (now - self.requests[0]) + self._adaptive_delay
                )
                if wait_time > 0:
                    logger.info(f"Rate limit reached. Waiting {wait_time:.2f} seconds")
                    time.sleep(wait_time)
                    now = time.time()

            # ì ì‘í˜• ì§€ì—° ì ìš©
            if self._adaptive_delay > 0:
                time.sleep(self._adaptive_delay)

            self.requests.append(now)
            return self._adaptive_delay

    def report_error(self):
        """API ì˜¤ë¥˜ ë³´ê³  - ì ì‘í˜• ì§€ì—° ì¦ê°€"""
        with self.lock:
            self._consecutive_errors += 1
            self._last_error_time = time.time()

            # ì—°ì† ì˜¤ë¥˜ì— ë”°ë¥¸ ì§€ì—° ì¦ê°€ (ìµœëŒ€ 10ì´ˆ)
            self._adaptive_delay = min(self._consecutive_errors * 0.5, 10.0)
            logger.warning(
                f"API error reported. Adaptive delay: {self._adaptive_delay:.2f}s"
            )

    def report_success(self):
        """API ì„±ê³µ ë³´ê³  - ì ì‘í˜• ì§€ì—° ê°ì†Œ"""
        with self.lock:
            if self._consecutive_errors > 0:
                self._consecutive_errors = max(0, self._consecutive_errors - 1)
                self._adaptive_delay = max(0, self._adaptive_delay - 0.1)


class CircuitBreaker:
    """ì„œí‚· ë¸Œë ˆì´ì»¤ íŒ¨í„´ êµ¬í˜„"""

    def __init__(self, failure_threshold: int = 5, timeout: int = 60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN
        self.lock = threading.Lock()

    def call(self, func: Callable, *args, **kwargs):
        """ì„œí‚· ë¸Œë ˆì´ì»¤ë¥¼ í†µí•œ í•¨ìˆ˜ í˜¸ì¶œ"""
        with self.lock:
            if self.state == "OPEN":
                if self._should_attempt_reset():
                    self.state = "HALF_OPEN"
                    logger.info("Circuit breaker: Attempting reset (HALF_OPEN)")
                else:
                    raise Exception("Circuit breaker is OPEN")

        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise e

    def _should_attempt_reset(self) -> bool:
        """ì¬ì‹œë„ ì‹œë„ ì—¬ë¶€ í™•ì¸"""
        return (
            self.last_failure_time
            and time.time() - self.last_failure_time >= self.timeout
        )

    def _on_success(self):
        """ì„±ê³µ ì‹œ ì²˜ë¦¬"""
        with self.lock:
            self.failure_count = 0
            if self.state == "HALF_OPEN":
                self.state = "CLOSED"
                logger.info("Circuit breaker: Reset to CLOSED")

    def _on_failure(self):
        """ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬"""
        with self.lock:
            self.failure_count += 1
            self.last_failure_time = time.time()

            if self.failure_count >= self.failure_threshold:
                self.state = "OPEN"
                logger.warning(
                    f"Circuit breaker: OPEN (failures: {self.failure_count})"
                )


class ResponseCache:
    """API ì‘ë‹µ ìºì‹± ì‹œìŠ¤í…œ"""

    def __init__(self, max_size: int = 1000, ttl_seconds: int = 300):
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.cache = {}
        self.access_times = {}
        self.lock = threading.Lock()

        # ìºì‹œ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.abspath(os.path.join(script_dir, "..", ".."))
        cache_dir = os.path.join(project_root, "cache", "api_responses")
        os.makedirs(cache_dir, exist_ok=True)
        self.cache_file = os.path.join(cache_dir, "response_cache.pkl")

        # ìºì‹œ ë¡œë“œ
        self._load_cache()

    def _generate_cache_key(self, request: APIRequest) -> str:
        """ìš”ì²­ ì •ë³´ë¡œë¶€í„° ìºì‹œ í‚¤ ìƒì„±"""
        key_data = {
            "api_name": request.api_name,
            "method": request.method,
            "tr_id": request.tr_id,
            "params": request.params,
            "body": request.body,
        }
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()

    def get(self, request: APIRequest) -> Optional[APIResponse]:
        """ìºì‹œì—ì„œ ì‘ë‹µ ì¡°íšŒ"""
        cache_key = request.cache_key or self._generate_cache_key(request)

        with self.lock:
            if cache_key in self.cache:
                cached_response, cached_time = self.cache[cache_key]

                # TTL í™•ì¸
                if time.time() - cached_time <= self.ttl_seconds:
                    self.access_times[cache_key] = time.time()
                    cached_response.cached = True
                    logger.debug(f"Cache hit for key: {cache_key[:8]}...")
                    return cached_response
                else:
                    # ë§Œë£Œëœ ìºì‹œ ì œê±°
                    del self.cache[cache_key]
                    if cache_key in self.access_times:
                        del self.access_times[cache_key]

        return None

    def put(self, request: APIRequest, response: APIResponse):
        """ì‘ë‹µì„ ìºì‹œì— ì €ì¥"""
        cache_key = request.cache_key or self._generate_cache_key(request)

        with self.lock:
            # ìºì‹œ í¬ê¸° ì œí•œ
            if len(self.cache) >= self.max_size:
                self._evict_oldest()

            self.cache[cache_key] = (response, time.time())
            self.access_times[cache_key] = time.time()
            logger.debug(f"Cached response for key: {cache_key[:8]}...")

            # ì£¼ê¸°ì ìœ¼ë¡œ ìºì‹œ ì €ì¥
            if len(self.cache) % 10 == 0:
                self._save_cache()

    def _evict_oldest(self):
        """ê°€ì¥ ì˜¤ë˜ëœ ìºì‹œ í•­ëª© ì œê±° (LRU)"""
        if not self.access_times:
            return

        oldest_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])

        if oldest_key in self.cache:
            del self.cache[oldest_key]
        del self.access_times[oldest_key]

    def _save_cache(self):
        """ìºì‹œë¥¼ íŒŒì¼ì— ì €ì¥"""
        try:
            with open(self.cache_file, "wb") as f:
                pickle.dump({"cache": self.cache, "access_times": self.access_times}, f)
        except Exception as e:
            logger.error(f"Failed to save cache: {e}")

    def _load_cache(self):
        """íŒŒì¼ì—ì„œ ìºì‹œ ë¡œë“œ"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, "rb") as f:
                    data = pickle.load(f)
                    self.cache = data.get("cache", {})
                    self.access_times = data.get("access_times", {})
                logger.info(f"Loaded {len(self.cache)} cached responses")
        except Exception as e:
            logger.error(f"Failed to load cache: {e}")
            self.cache = {}
            self.access_times = {}

    def clear(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        with self.lock:
            self.cache.clear()
            self.access_times.clear()
        try:
            if os.path.exists(self.cache_file):
                os.remove(self.cache_file)
        except Exception as e:
            logger.error(f"Failed to remove cache file: {e}")


class DateRangeSplitter:
    """ë™ì  ë‚ ì§œ ë²”ìœ„ ë¶„í• ê¸°"""

    def __init__(self):
        self.code_statistics = defaultdict(dict)
        self._load_statistics()

    def calculate_optimal_splits(
        self,
        start_date: datetime,
        end_date: datetime,
        code: str,
        data_type: str = "daily",
    ) -> List[Tuple[datetime, datetime]]:
        """ì½”ë“œì™€ ë°ì´í„° íƒ€ì…ì— ë”°ë¥¸ ìµœì  ë¶„í•  ê³„ì‚°"""

        total_days = (end_date - start_date).days

        # ì½”ë“œë³„ í†µê³„ ê¸°ë°˜ ë¶„í•  í¬ê¸° ê²°ì •
        avg_records_per_day = self._get_avg_records_per_day(code, data_type)

        if avg_records_per_day > 50:  # ê³ ë¹ˆë„ ë°ì´í„°
            split_days = 30  # 1ê°œì›” ë‹¨ìœ„
        elif avg_records_per_day > 10:  # ì¤‘ë¹ˆë„ ë°ì´í„°
            split_days = 90  # 3ê°œì›” ë‹¨ìœ„
        else:  # ì €ë¹ˆë„ ë°ì´í„°
            split_days = 180  # 6ê°œì›” ë‹¨ìœ„

        # ìµœëŒ€ ë¶„í•  ìˆ˜ ì œí•œ (API í˜¸ì¶œ ìµœì†Œí™”)
        max_splits = 10
        if total_days // split_days > max_splits:
            split_days = total_days // max_splits

        # ë‚ ì§œ ë²”ìœ„ ë¶„í• 
        splits = []
        current_date = start_date

        while current_date < end_date:
            split_end = min(current_date + timedelta(days=split_days), end_date)
            splits.append((current_date, split_end))
            current_date = split_end + timedelta(days=1)

        logger.info(
            f"Date range split for {code}: {len(splits)} segments "
            f"(avg {avg_records_per_day:.1f} records/day)"
        )

        return splits

    def _get_avg_records_per_day(self, code: str, data_type: str) -> float:
        """ì½”ë“œë³„ í‰ê·  ì¼ì¼ ë ˆì½”ë“œ ìˆ˜ ì¡°íšŒ"""
        stats = self.code_statistics.get(code, {}).get(data_type, {})
        return stats.get("avg_records_per_day", 1.0)

    def update_statistics(
        self, code: str, data_type: str, date_range_days: int, record_count: int
    ):
        """ì½”ë“œë³„ í†µê³„ ì—…ë°ì´íŠ¸"""
        if code not in self.code_statistics:
            self.code_statistics[code] = {}
        if data_type not in self.code_statistics[code]:
            self.code_statistics[code][data_type] = {
                "total_records": 0,
                "total_days": 0,
                "avg_records_per_day": 1.0,
            }

        stats = self.code_statistics[code][data_type]
        stats["total_records"] += record_count
        stats["total_days"] += date_range_days
        stats["avg_records_per_day"] = stats["total_records"] / max(
            stats["total_days"], 1
        )

        # ì£¼ê¸°ì ìœ¼ë¡œ í†µê³„ ì €ì¥
        if stats["total_records"] % 100 == 0:
            self._save_statistics()

    def _save_statistics(self):
        """í†µê³„ë¥¼ íŒŒì¼ì— ì €ì¥"""
        try:
            script_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.abspath(os.path.join(script_dir, "..", ".."))
            cache_dir = os.path.join(project_root, "cache", "statistics")
            os.makedirs(cache_dir, exist_ok=True)

            stats_file = os.path.join(cache_dir, "code_statistics.json")
            with open(stats_file, "w", encoding="utf-8") as f:
                json.dump(self.code_statistics, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"Failed to save statistics: {e}")

    def _load_statistics(self):
        """íŒŒì¼ì—ì„œ í†µê³„ ë¡œë“œ"""
        try:
            script_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.abspath(os.path.join(script_dir, "..", ".."))
            stats_file = os.path.join(
                project_root, "cache", "statistics", "code_statistics.json"
            )

            if os.path.exists(stats_file):
                with open(stats_file, "r", encoding="utf-8") as f:
                    self.code_statistics = defaultdict(dict, json.load(f))
                logger.info(f"Loaded statistics for {len(self.code_statistics)} codes")
        except Exception as e:
            logger.error(f"Failed to load statistics: {e}")
            self.code_statistics = defaultdict(dict)


class PerformanceMonitor:
    """API ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""

    def __init__(self):
        self.metrics = defaultdict(list)
        self.lock = threading.Lock()

    def record_request(
        self, api_name: str, response_time: float, success: bool, cached: bool = False
    ):
        """ìš”ì²­ ì„±ëŠ¥ ê¸°ë¡"""
        with self.lock:
            timestamp = time.time()
            self.metrics[api_name].append(
                {
                    "timestamp": timestamp,
                    "response_time": response_time,
                    "success": success,
                    "cached": cached,
                }
            )

            # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ìµœê·¼ 1000ê°œë§Œ ìœ ì§€
            if len(self.metrics[api_name]) > 1000:
                self.metrics[api_name] = self.metrics[api_name][-1000:]

    def get_api_stats(self, api_name: str, time_window: int = 3600) -> Dict[str, Any]:
        """API í†µê³„ ì¡°íšŒ"""
        with self.lock:
            if api_name not in self.metrics:
                return {}

            now = time.time()
            recent_metrics = [
                m for m in self.metrics[api_name] if now - m["timestamp"] <= time_window
            ]

            if not recent_metrics:
                return {}

            response_times = [m["response_time"] for m in recent_metrics]
            success_count = sum(1 for m in recent_metrics if m["success"])
            cache_hit_count = sum(1 for m in recent_metrics if m["cached"])

            return {
                "total_requests": len(recent_metrics),
                "success_rate": success_count / len(recent_metrics),
                "cache_hit_rate": cache_hit_count / len(recent_metrics),
                "avg_response_time": sum(response_times) / len(response_times),
                "min_response_time": min(response_times),
                "max_response_time": max(response_times),
            }

    def get_overall_stats(self) -> Dict[str, Any]:
        """ì „ì²´ ì„±ëŠ¥ í†µê³„"""
        with self.lock:
            stats = {}
            for api_name in self.metrics.keys():
                stats[api_name] = self.get_api_stats(api_name)
            return stats


class APIOptimizer:
    """í†µí•© API ìµœì í™” ê´€ë¦¬ì"""

    def __init__(
        self,
        api_client,
        max_requests_per_minute: int = 60,
        cache_ttl_seconds: int = 300,
        max_workers: int = 3,
    ):
        """
        API ìµœì í™” ê´€ë¦¬ì ì´ˆê¸°í™”

        Args:
            api_client: ê¸°ì¡´ API í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
            max_requests_per_minute: ë¶„ë‹¹ ìµœëŒ€ ìš”ì²­ ìˆ˜
            cache_ttl_seconds: ìºì‹œ TTL (ì´ˆ)
            max_workers: ìµœëŒ€ ì‘ì—…ì ìŠ¤ë ˆë“œ ìˆ˜
        """
        self.api_client = api_client
        self.max_workers = max_workers

        # ìµœì í™” ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™”
        self.rate_limiter = RateLimiter(max_requests_per_minute, 60)
        self.circuit_breaker = CircuitBreaker(failure_threshold=5, timeout=60)
        self.response_cache = ResponseCache(
            max_size=1000, ttl_seconds=cache_ttl_seconds
        )
        self.date_splitter = DateRangeSplitter()
        self.performance_monitor = PerformanceMonitor()

        # ìš”ì²­ í ë° ë°°ì¹˜ ì²˜ë¦¬
        self.request_queue = deque()
        self.batch_size = 10
        self.batch_timeout = 5.0
        self._last_batch_time = time.time()
        self._processing = False
        self._stop_event = threading.Event()

        # ë°°ì¹˜ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘
        self._batch_thread = threading.Thread(target=self._batch_processor, daemon=True)
        self._batch_thread.start()

        logger.info("ğŸš€ API Optimizer initialized with advanced features")

    def optimized_request(
        self,
        api_name: str,
        method: str = "GET",
        tr_id: Optional[str] = None,
        params: Optional[Dict[str, Any]] = None,
        body: Optional[Dict[str, Any]] = None,
        headers: Optional[Dict[str, Any]] = None,
        priority: int = 5,
        use_cache: bool = True,
    ) -> APIResponse:
        """
        ìµœì í™”ëœ API ìš”ì²­ ìˆ˜í–‰

        Args:
            api_name: API ì´ë¦„
            method: HTTP ë©”ì†Œë“œ
            tr_id: ê±°ë˜ ID
            params: URL íŒŒë¼ë¯¸í„°
            body: ìš”ì²­ ë³¸ë¬¸
            headers: ì¶”ê°€ í—¤ë”
            priority: ìš°ì„ ìˆœìœ„ (1=ìµœê³ , 10=ìµœì €)
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€

        Returns:
            APIResponse: ìµœì í™”ëœ ì‘ë‹µ
        """
        # API ìš”ì²­ ê°ì²´ ìƒì„±
        request = APIRequest(
            api_name=api_name,
            method=method,
            tr_id=tr_id,
            params=params,
            body=body,
            headers=headers,
            priority=priority,
        )

        # ìºì‹œ í™•ì¸ (ì‚¬ìš©í•˜ëŠ” ê²½ìš°)
        if use_cache:
            cached_response = self.response_cache.get(request)
            if cached_response:
                self.performance_monitor.record_request(
                    api_name, 0.0, True, cached=True
                )
                logger.debug(f"ğŸ¯ Cache hit for {api_name}")
                return cached_response

        # ì‹¤ì œ API í˜¸ì¶œ ìˆ˜í–‰
        start_time = time.time()
        response = None
        success = False

        try:
            # ì†ë„ ì œí•œ ì ìš©
            self.rate_limiter.acquire()

            # ì„œí‚· ë¸Œë ˆì´ì»¤ë¥¼ í†µí•œ API í˜¸ì¶œ
            result = self.circuit_breaker.call(self._execute_api_request, request)

            # ì‘ë‹µ ê°ì²´ ìƒì„±
            response = APIResponse(
                request=request,
                data=result,
                status_code=200,
                response_time=time.time() - start_time,
                cached=False,
            )

            success = True
            self.rate_limiter.report_success()

            # ìºì‹œì— ì €ì¥ (ì„±ê³µí•œ ê²½ìš°ë§Œ)
            if use_cache and result.get("rt_cd") == "0":
                self.response_cache.put(request, response)

            logger.debug(
                f"âœ… API request completed: {api_name} "
                f"({response.response_time:.2f}s)"
            )

        except Exception as e:
            self.rate_limiter.report_error()

            # ì˜¤ë¥˜ ì‘ë‹µ ìƒì„±
            response = APIResponse(
                request=request,
                data={
                    "rt_cd": "1",
                    "msg1": str(e),
                    "output": {},
                    "output1": [],
                    "output2": [],
                },
                status_code=500,
                response_time=time.time() - start_time,
                cached=False,
                error=str(e),
            )

            logger.error(f"âŒ API request failed: {api_name} - {e}")

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡
        self.performance_monitor.record_request(
            api_name, response.response_time, success, cached=False
        )

        return response

    def _execute_api_request(self, request: APIRequest) -> Dict[str, Any]:
        """ì‹¤ì œ API ìš”ì²­ ì‹¤í–‰"""
        return self.api_client.request(
            method=request.method,
            api_name=request.api_name,
            tr_id=request.tr_id,
            params=request.params,
            body=request.body,
            headers=request.headers,
        )

    def batch_request(
        self, requests: List[APIRequest], max_workers: Optional[int] = None
    ) -> List[APIResponse]:
        """ë°°ì¹˜ API ìš”ì²­ ì²˜ë¦¬"""
        max_workers = max_workers or self.max_workers
        responses = []

        # ìš°ì„ ìˆœìœ„ë³„ë¡œ ì •ë ¬
        sorted_requests = sorted(requests, key=lambda r: r.priority)

        logger.info(f"ğŸ”„ Processing batch of {len(requests)} requests")

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ë™ì‹œ ì‹¤í–‰
            future_to_request = {
                executor.submit(
                    self.optimized_request,
                    req.api_name,
                    req.method,
                    req.tr_id,
                    req.params,
                    req.body,
                    req.headers,
                    req.priority,
                ): req
                for req in sorted_requests
            }

            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_request):
                request = future_to_request[future]
                try:
                    response = future.result()
                    responses.append(response)
                except Exception as e:
                    logger.error(f"Batch request failed for {request.api_name}: {e}")
                    error_response = APIResponse(
                        request=request,
                        data={"rt_cd": "1", "msg1": str(e)},
                        status_code=500,
                        response_time=0.0,
                        error=str(e),
                    )
                    responses.append(error_response)

        # ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        request_order = {id(req): i for i, req in enumerate(requests)}
        responses.sort(key=lambda r: request_order.get(id(r.request), 999))

        logger.info(f"âœ… Batch processing completed: {len(responses)} responses")
        return responses

    def optimize_date_range_requests(
        self,
        api_name: str,
        code: str,
        start_date: datetime,
        end_date: datetime,
        base_params: Dict[str, Any],
        date_param_name: str = "start_date",
        end_date_param_name: str = "end_date",
        tr_id: Optional[str] = None,
    ) -> List[APIResponse]:
        """ë‚ ì§œ ë²”ìœ„ ê¸°ë°˜ ìµœì í™”ëœ ìš”ì²­ ì²˜ë¦¬"""

        # ìµœì  ë¶„í•  ê³„ì‚°
        date_ranges = self.date_splitter.calculate_optimal_splits(
            start_date, end_date, code, "daily"
        )

        # ìš”ì²­ ìƒì„±
        requests = []
        for range_start, range_end in date_ranges:
            params = base_params.copy()
            params[date_param_name] = range_start.strftime("%Y%m%d")
            params[end_date_param_name] = range_end.strftime("%Y%m%d")

            request = APIRequest(
                api_name=api_name, tr_id=tr_id, params=params, priority=5
            )
            requests.append(request)

        logger.info(f"ğŸ“… Date range optimized: {len(requests)} API calls for {code}")

        # ë°°ì¹˜ ì²˜ë¦¬
        responses = self.batch_request(requests)

        # í†µê³„ ì—…ë°ì´íŠ¸
        total_records = 0
        for response in responses:
            if response.data.get("rt_cd") == "0":
                output_data = response.data.get("output2", [])
                total_records += len(output_data)

        total_days = (end_date - start_date).days
        self.date_splitter.update_statistics(code, "daily", total_days, total_records)

        return responses

    def _batch_processor(self):
        """ë°°ê²½ì—ì„œ ì‹¤í–‰ë˜ëŠ” ë°°ì¹˜ ì²˜ë¦¬ê¸°"""
        while not self._stop_event.is_set():
            try:
                current_time = time.time()

                # ë°°ì¹˜ ì²˜ë¦¬ ì¡°ê±´ í™•ì¸
                if len(self.request_queue) >= self.batch_size or (
                    self.request_queue
                    and current_time - self._last_batch_time >= self.batch_timeout
                ):

                    # íì—ì„œ ìš”ì²­ë“¤ ì¶”ì¶œ
                    batch_requests = []
                    while self.request_queue and len(batch_requests) < self.batch_size:
                        batch_requests.append(self.request_queue.popleft())

                    if batch_requests:
                        logger.debug(
                            f"ğŸ”„ Processing batch: {len(batch_requests)} requests"
                        )
                        self.batch_request(batch_requests)
                        self._last_batch_time = current_time

                time.sleep(0.1)  # CPU ì‚¬ìš©ëŸ‰ ì¡°ì ˆ

            except Exception as e:
                logger.error(f"Batch processor error: {e}")
                time.sleep(1)

    def get_performance_report(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        stats = self.performance_monitor.get_overall_stats()

        # ì „ì²´ í†µê³„ ê³„ì‚°
        total_requests = sum(s.get("total_requests", 0) for s in stats.values())
        avg_success_rate = sum(s.get("success_rate", 0) for s in stats.values()) / max(
            len(stats), 1
        )
        avg_cache_hit_rate = sum(
            s.get("cache_hit_rate", 0) for s in stats.values()
        ) / max(len(stats), 1)

        return {
            "summary": {
                "total_apis": len(stats),
                "total_requests": total_requests,
                "avg_success_rate": avg_success_rate,
                "avg_cache_hit_rate": avg_cache_hit_rate,
                "circuit_breaker_state": self.circuit_breaker.state,
                "current_rate_limit_delay": self.rate_limiter._adaptive_delay,
            },
            "api_details": stats,
            "timestamp": datetime.now().isoformat(),
        }

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self._stop_event.set()
        if self._batch_thread.is_alive():
            self._batch_thread.join(timeout=5)

        # ìºì‹œ ì €ì¥
        self.response_cache._save_cache()
        self.date_splitter._save_statistics()

        logger.info("ğŸ§¹ API Optimizer cleanup completed")

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()
