"""
ì¦ë¶„ ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤

ì´ ëª¨ë“ˆì€ ë‹¤ì–‘í•œ í”¼ì²˜ í´ë˜ìŠ¤ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”
ì¦ë¶„ ì—…ë°ì´íŠ¸ ê´€ë ¨ ê³µí†µ í•¨ìˆ˜ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import logging
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, Union
import pandas as pd
from datetime import datetime

logger = logging.getLogger(__name__)


def save_feature_to_csv_incremental(
    data_dict: Dict[str, pd.DataFrame],
    base_path: Path,
    feature_name: str,
    metadata_manager,
    incremental_mode: bool = True,
    date_column: str = "trade_date",
    time_column: Optional[str] = None,
    backup_enabled: bool = True,
) -> Dict[str, Any]:
    """
    í”¼ì²˜ ë°ì´í„°ë¥¼ ì¦ë¶„ ëª¨ë“œë¡œ CSVì— ì €ì¥

    Args:
        data_dict: {code: dataframe} í˜•íƒœì˜ ë°ì´í„°
        base_path: ì €ì¥í•  ê¸°ë³¸ ê²½ë¡œ
        feature_name: í”¼ì²˜ëª…
        metadata_manager: MetadataManager ì¸ìŠ¤í„´ìŠ¤
        incremental_mode: ì¦ë¶„ ëª¨ë“œ ì—¬ë¶€
        date_column: ë‚ ì§œ ì»¬ëŸ¼ëª…
        time_column: ì‹œê°„ ì»¬ëŸ¼ëª… (ì„ íƒ)
        backup_enabled: ë°±ì—… ìƒì„± ì—¬ë¶€

    Returns:
        Dict: ì €ì¥ ê²°ê³¼ ë° í†µê³„
    """
    results = {
        "success_count": 0,
        "error_count": 0,
        "total_files": len(data_dict),
        "saved_files": [],
        "errors": [],
        "stats": {
            "total_new_records": 0,
            "total_existing_records": 0,
            "total_merged_records": 0,
        },
    }

    # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    save_dir = base_path / feature_name
    save_dir.mkdir(parents=True, exist_ok=True)

    for code, df in data_dict.items():
        try:
            # CSV íŒŒì¼ ê²½ë¡œ
            csv_path = save_dir / f"{code}.csv"

            if not incremental_mode:
                # ì „ì²´ ë®ì–´ì“°ê¸° ëª¨ë“œ
                df.to_csv(csv_path, index=False, encoding="utf-8-sig")
                results["saved_files"].append(str(csv_path))
                results["success_count"] += 1
                results["stats"]["total_new_records"] += len(df)

                # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                metadata_manager.update_metadata_incremental(
                    feature_name, code, csv_path, len(df), ("", ""), date_column
                )

                logger.info(
                    f"âœ… {feature_name}/{code}: ì „ì²´ ë®ì–´ì“°ê¸° ì €ì¥ ì™„ë£Œ ({len(df)}ê±´)"
                )
                continue

            # ì¦ë¶„ ëª¨ë“œ ì²˜ë¦¬
            backup_path = None

            try:
                # 1. ê¸°ì¡´ ë°ì´í„° ë°±ì—… (ì„ íƒì‚¬í•­)
                if backup_enabled and csv_path.exists():
                    backup_path = metadata_manager.backup_csv_file(csv_path)

                # 2. ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„° í•©ì¹˜ê¸°
                existing_df = pd.DataFrame()
                if csv_path.exists():
                    existing_df = pd.read_csv(csv_path)

                merged_df = metadata_manager.merge_csv_data(
                    csv_path, df, date_column, time_column
                )

                # 3. ë°ì´í„° ê²€ì¦
                validation_result = metadata_manager.validate_merged_data(
                    existing_df, df, merged_df, date_column
                )

                # ê²€ì¦ ì‹¤íŒ¨ì‹œ ê²½ê³  ë¡œê·¸
                if not validation_result["is_valid"]:
                    logger.error(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {feature_name}/{code}")
                    for error in validation_result["errors"]:
                        logger.error(f"  - {error}")

                if validation_result["warnings"]:
                    for warning in validation_result["warnings"]:
                        logger.warning(f"  - {warning}")

                # 4. í•©ì³ì§„ ë°ì´í„° ì €ì¥
                merged_df.to_csv(csv_path, index=False, encoding="utf-8-sig")

                # 5. ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                new_records = len(df)
                date_range = ("", "")  # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì ì ˆí•œ ë‚ ì§œ ë²”ìœ„ ì„¤ì •

                metadata_manager.update_metadata_incremental(
                    feature_name, code, csv_path, new_records, date_range, date_column
                )

                # 6. í†µê³„ ì—…ë°ì´íŠ¸
                stats = validation_result["stats"]
                results["stats"]["total_new_records"] += stats["new_records"]
                results["stats"]["total_existing_records"] += stats["old_records"]
                results["stats"]["total_merged_records"] += stats["merged_records"]

                results["saved_files"].append(str(csv_path))
                results["success_count"] += 1

                logger.info(
                    f"âœ… {feature_name}/{code}: ì¦ë¶„ ì €ì¥ ì™„ë£Œ "
                    f"(ê¸°ì¡´: {stats['old_records']}ê±´, "
                    f"ì‹ ê·œ: {stats['new_records']}ê±´, "
                    f"ìµœì¢…: {stats['merged_records']}ê±´)"
                )

                # ë°±ì—… íŒŒì¼ ì •ë¦¬ (ì„±ê³µì‹œ)
                if backup_path and backup_path.exists():
                    backup_path.unlink()

            except Exception as save_error:
                # ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒì‹œ ë¡¤ë°±
                if backup_path:
                    logger.warning(f"ì €ì¥ ì˜¤ë¥˜ë¡œ ë¡¤ë°± ì‹œë„: {feature_name}/{code}")
                    metadata_manager.rollback_from_backup(csv_path, backup_path)

                raise save_error

        except Exception as e:
            error_msg = f"{feature_name}/{code} ì €ì¥ ì‹¤íŒ¨: {str(e)}"
            logger.error(error_msg)
            results["errors"].append(error_msg)
            results["error_count"] += 1

    return results


def get_dynamic_date_range(
    metadata_manager,
    feature_path: str,
    codes: list,
    default_start: str = "20240101",
    default_end: Optional[str] = None,
    max_days_back: int = 90,
) -> Dict[str, Tuple[str, str]]:
    """
    ê° ì½”ë“œë³„ë¡œ ë™ì  ë‚ ì§œ ë²”ìœ„ ê³„ì‚°

    Args:
        metadata_manager: MetadataManager ì¸ìŠ¤í„´ìŠ¤
        feature_path: í”¼ì²˜ ê²½ë¡œ
        codes: ì²˜ë¦¬í•  ì½”ë“œ ë¦¬ìŠ¤íŠ¸
        default_start: ê¸°ë³¸ ì‹œì‘ì¼
        default_end: ê¸°ë³¸ ì¢…ë£Œì¼ (Noneì´ë©´ ì˜¤ëŠ˜)
        max_days_back: ìµœëŒ€ ë©°ì¹  ì „ê¹Œì§€ í—ˆìš©í• ì§€

    Returns:
        Dict: {code: (start_date, end_date)} í˜•íƒœ
    """
    if default_end is None:
        default_end = datetime.now().strftime("%Y%m%d")

    date_ranges = {}

    for code in codes:
        try:
            start_date, end_date = metadata_manager.calculate_incremental_range(
                feature_path, code, max_days_back
            )

            if start_date is None:
                # ì „ì²´ ìˆ˜ì§‘ í•„ìš”
                date_ranges[code] = (default_start, default_end)
                logger.info(
                    f"ì „ì²´ ìˆ˜ì§‘: {feature_path}/{code} ({default_start}~{default_end})"
                )
            else:
                # ì¦ë¶„ ìˆ˜ì§‘
                date_ranges[code] = (start_date, end_date)
                if start_date <= end_date:
                    logger.info(
                        f"ì¦ë¶„ ìˆ˜ì§‘: {feature_path}/{code} ({start_date}~{end_date})"
                    )
                else:
                    logger.info(f"ìˆ˜ì§‘ ë¶ˆí•„ìš”: {feature_path}/{code} (ìµœì‹  ìƒíƒœ)")

        except Exception as e:
            logger.error(f"ë‚ ì§œ ë²”ìœ„ ê³„ì‚° ì˜¤ë¥˜ {feature_path}/{code}: {e}")
            # ì˜¤ë¥˜ì‹œ ê¸°ë³¸ ë²”ìœ„ ì‚¬ìš©
            date_ranges[code] = (default_start, default_end)

    return date_ranges


def should_update_data(
    metadata_manager, feature_path: str, code: str, max_age_hours: int = 24
) -> bool:
    """
    ë°ì´í„° ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•œì§€ íŒë‹¨

    Args:
        metadata_manager: MetadataManager ì¸ìŠ¤í„´ìŠ¤
        feature_path: í”¼ì²˜ ê²½ë¡œ
        code: ì½”ë“œ
        max_age_hours: ìµœëŒ€ ë°ì´í„° ìœ íš¨ ì‹œê°„ (ì‹œê°„)

    Returns:
        bool: ì—…ë°ì´íŠ¸ í•„ìš” ì—¬ë¶€
    """
    try:
        last_info = metadata_manager.load_last_update_info(feature_path, code)

        if not last_info:
            return True  # ë©”íƒ€ë°ì´í„° ì—†ìœ¼ë©´ ì—…ë°ì´íŠ¸ í•„ìš”

        last_update = last_info.get("last_update_timestamp")
        if not last_update:
            return True

        # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ í™•ì¸
        last_update_dt = datetime.fromisoformat(last_update.replace("Z", "+00:00"))
        hours_passed = (
            datetime.now() - last_update_dt.replace(tzinfo=None)
        ).total_seconds() / 3600

        if hours_passed > max_age_hours:
            logger.info(
                f"ë°ì´í„° ê°±ì‹  í•„ìš”: {feature_path}/{code} ({hours_passed:.1f}ì‹œê°„ ê²½ê³¼)"
            )
            return True
        else:
            logger.info(
                f"ë°ì´í„° ìµœì‹  ìƒíƒœ: {feature_path}/{code} ({hours_passed:.1f}ì‹œê°„ ê²½ê³¼)"
            )
            return False

    except Exception as e:
        logger.error(f"ì—…ë°ì´íŠ¸ í•„ìš”ì„± íŒë‹¨ ì˜¤ë¥˜: {e}")
        return True  # ì˜¤ë¥˜ì‹œ ì•ˆì „í•˜ê²Œ ì—…ë°ì´íŠ¸ ì‹¤í–‰


def log_incremental_summary(results: Dict[str, Any]) -> None:
    """
    ì¦ë¶„ ì—…ë°ì´íŠ¸ ê²°ê³¼ ìš”ì•½ ë¡œê¹…

    Args:
        results: save_feature_to_csv_incremental ê²°ê³¼
    """
    stats = results["stats"]

    logger.info("=" * 60)
    logger.info("ğŸ“Š ì¦ë¶„ ì—…ë°ì´íŠ¸ ìš”ì•½")
    logger.info("=" * 60)
    logger.info(f"ğŸ“ ì²˜ë¦¬ íŒŒì¼: {results['total_files']}ê°œ")
    logger.info(f"âœ… ì„±ê³µ: {results['success_count']}ê°œ")
    logger.info(f"âŒ ì‹¤íŒ¨: {results['error_count']}ê°œ")
    logger.info(f"ğŸ“ˆ ê¸°ì¡´ ë ˆì½”ë“œ: {stats['total_existing_records']:,}ê±´")
    logger.info(f"ğŸ†• ì‹ ê·œ ë ˆì½”ë“œ: {stats['total_new_records']:,}ê±´")
    logger.info(f"ğŸ”„ ìµœì¢… ë ˆì½”ë“œ: {stats['total_merged_records']:,}ê±´")

    if results["errors"]:
        logger.error("âŒ ì˜¤ë¥˜ ëª©ë¡:")
        for error in results["errors"]:
            logger.error(f"  - {error}")

    logger.info("=" * 60)
